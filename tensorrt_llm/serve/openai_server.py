#!/usr/bin/env python
import asyncio
import json
import os
import re
import signal
import time
import traceback
from contextlib import asynccontextmanager
from datetime import datetime
from http import HTTPStatus
from pathlib import Path
from typing import Any, AsyncGenerator, AsyncIterator, List, Optional

import uvicorn
from fastapi import FastAPI, Request
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse, Response, StreamingResponse
from starlette.routing import Mount
from transformers import AutoConfig, AutoProcessor

from tensorrt_llm._tensorrt_engine import LLM
# yapf: disable
from tensorrt_llm.executor import CppExecutorError
from tensorrt_llm.executor.postproc_worker import PostprocParams
from tensorrt_llm.inputs import prompt_inputs
from tensorrt_llm.inputs.utils import ConversationMessage, apply_chat_template
from tensorrt_llm.llmapi import DisaggregatedParams as LlmDisaggregatedParams
from tensorrt_llm.llmapi.disagg_utils import MetadataServerConfig, ServerRole
from tensorrt_llm.llmapi.llm import RequestOutput
from tensorrt_llm.logger import logger
from tensorrt_llm.metrics.collector import MetricsCollector
from tensorrt_llm.serve.chat_utils import (check_multiple_response,
                                           parse_chat_messages_coroutines)
from tensorrt_llm.serve.metadata_server import create_metadata_server
from tensorrt_llm.serve.openai_protocol import (
    ChatCompletionRequest, ChatCompletionResponse,
    ChatCompletionResponseStreamChoice, ChatCompletionStreamResponse,
    CompletionRequest, CompletionResponse, CompletionResponseChoice,
    DeltaMessage, ErrorResponse, ModelCard, ModelList, ResponsesRequest,
    ResponsesResponse, UsageInfo, to_llm_disaggregated_params)
from tensorrt_llm.serve.postprocess_handlers import (
    ChatPostprocArgs, CompletionPostprocArgs, chat_response_post_processor,
    chat_stream_post_processor, completion_response_post_processor,
    completion_stream_post_processor)
from tensorrt_llm.version import __version__ as VERSION

from .._utils import nvtx_mark, set_prometheus_multiproc_dir
from .harmony_adapter import HarmonyAdapter
from .responses_utils import (ConversationHistoryStore,
                              construct_harmony_messages, decode_tokens,
                              parse_output_message, parse_output_tokens,
                              render_for_completion)

# yapf: enale
TIMEOUT_KEEP_ALIVE = 5  # seconds.


class OpenAIServer:

    def __init__(self,
                 llm: LLM,
                 model: str,
                 server_role: Optional[ServerRole],
                 metadata_server_cfg: MetadataServerConfig):
        self.llm = llm
        self.tokenizer = llm.tokenizer
        self.metadata_server = create_metadata_server(metadata_server_cfg)
        self.server_role = server_role
        self.binding_addr = None  # Will be set in __call__
        hf_tokenizer_path = llm._hf_model_dir or self.tokenizer.tokenizer.name_or_path
        trust_remote_code = llm.args.trust_remote_code
        try:
            self.processor = AutoProcessor.from_pretrained(hf_tokenizer_path, trust_remote_code=trust_remote_code)
        except Exception:
            logger.debug("Failed to load AutoProcessor or AutoConfig for %s", hf_tokenizer_path)
            self.processor = None
        try:
            self.model_config = AutoConfig.from_pretrained(hf_tokenizer_path, trust_remote_code=trust_remote_code)
        except Exception:
            logger.debug("Failed to load AutoConfig for %s", hf_tokenizer_path)
            self.model_config = None

        # Enable response storage for Responses API
        self.enable_store = True
        if len(os.getenv("TRTLLM_RESPONSES_API_DISABLE_STORE", "")) > 0:
            self.enable_store = False
        self.conversation_store = ConversationHistoryStore()

        model_dir = Path(model)
        if model_dir.exists() and model_dir.is_dir():
            self.model = model_dir.name
        else:
            self.model = model
        self.metrics_collector = None
        if self.llm.args.return_perf_metrics:
            set_prometheus_multiproc_dir()
            self.metrics_collector = MetricsCollector({
                "model_name": "undefined",
                "engine_type": "undefined"
            })

        # gpt-oss
        self.harmony_adapter = HarmonyAdapter()
        self.use_harmony = self.model_config.model_type == "gpt_oss"

        @asynccontextmanager
        async def lifespan(app: FastAPI):
            if self.metadata_server is not None:
                metadata = {
                    "model": self.model,
                    "version": VERSION,
                    "timestamp": datetime.now().isoformat(),
                    "server_role": server_role.name,
                    "url": self.binding_addr
                }
                # TODO: add more metadata
                # Register with ETCD using the existing key format
                self.metadata_server.put(f"trtllm/{self.llm.llm_id}", metadata)
                logger.info(f"trtllm/{self.llm.llm_id} is registered")

            # terminate rank0 worker
            yield

            if self.metadata_server is not None:
                self.metadata_server.remove(f"trtllm/{self.llm.llm_id}")
                logger.info(f"trtllm/{self.llm.llm_id} is unregistered")
            self.llm.shutdown()

        self.app = FastAPI(lifespan=lifespan)

        @self.app.exception_handler(RequestValidationError)
        async def validation_exception_handler(_, exc):
            return self.create_error_response(message=str(exc))

        self.register_routes()

    async def await_disconnected(self, raw_request: Request, promise):
        if raw_request is None:
            return
        while not await raw_request.is_disconnected():
            await asyncio.sleep(1)
        if not promise.finished:
            promise.abort()
            logger.info(
                f"{raw_request.client} is disconnected, abort {promise.request_id}")

    @property
    def postproc_worker_enabled(self) -> bool:
        return True if self.llm.args.num_postprocess_workers > 0 else False

    @staticmethod
    def create_error_response(
            message: str,
            err_type: str = "BadRequestError",
            status_code: HTTPStatus = HTTPStatus.BAD_REQUEST) -> Response:
        error_response = ErrorResponse(message=message,
                                       type=err_type,
                                       code=status_code.value)
        return JSONResponse(content=error_response.model_dump(),
                            status_code=error_response.code)

    def _create_invalid_response_id_error(self, response_id: str) -> Response:
        return self.create_error_response(
            err_type="InvalidRequestError",
            message=(f"Invalid 'response_id': '{response_id}'. "
                     "Expected an ID that begins with 'resp'."),
        )

    def _create_response_id_not_found_error(self, response_id: str) -> Response:
        return self.create_error_response(
            err_type="InvalidRequestError",
            message=f"Response with id '{response_id}' not found.",
            status_code=HTTPStatus.NOT_FOUND,
        )

    def register_routes(self):
        self.app.add_api_route("/health", self.health, methods=["GET"])
        self.app.add_api_route("/health_generate", self.health_generate, methods=["GET"])
        self.app.add_api_route("/version", self.version, methods=["GET"])
        self.app.add_api_route("/v1/models", self.get_model, methods=["GET"])
        # TODO: the metrics endpoint only reports iteration stats, not the runtime stats for now
        self.app.add_api_route("/metrics", self.get_iteration_stats, methods=["GET"])
        # TODO: workaround before ETCD support
        self.app.add_api_route("/kv_cache_events", self.get_kv_cache_events, methods=["POST"])
        self.app.add_api_route("/v1/completions",
                               self.openai_completion,
                               methods=["POST"])
        self.app.add_api_route("/v1/chat/completions",
                               self.openai_chat if not self.use_harmony else self.chat_harmony,
                               methods=["POST"])
        self.app.add_api_route("/v1/responses",
                               self.openai_responses,
                               methods=["POST"])
        if self.llm.args.return_perf_metrics:
            # register /prometheus/metrics
            self.mount_metrics()

    def mount_metrics(self):
        # Lazy import for prometheus multiprocessing.
        # We need to set PROMETHEUS_MULTIPROC_DIR environment variable
        # before prometheus_client is imported.
        # See https://prometheus.github.io/client_python/multiprocess/
        from prometheus_client import (CollectorRegistry, make_asgi_app,
                                       multiprocess)
        from prometheus_fastapi_instrumentator import Instrumentator
        registry = CollectorRegistry()
        multiprocess.MultiProcessCollector(registry)
        Instrumentator(
            should_group_status_codes=False,
            should_respect_env_var=True,
            excluded_handlers=[
                ".*"
            ],
            registry=registry,
        ).add().instrument(self.app).expose(self.app)
        metrics_app = make_asgi_app(registry=registry)
        metrics_route = Mount("/prometheus/metrics", metrics_app)
        metrics_route.path_regex = re.compile("^/prometheus/metrics(?P<path>.*)$")
        self.app.routes.append(metrics_route)

    async def health(self) -> Response:
        return Response(status_code=200)

    async def health_generate(self) -> Response:
        """Health check that performs a minimal generation."""
        try:
            # Create a minimal chat request
            health_request = ChatCompletionRequest(
                messages=[{"role": "user", "content": "hi"}], # Minimal prompt (often > 1 token after tokenization)
                model=self.model,
                max_completion_tokens=1, # Request only 1 token out
                stream=False,
                temperature=0.0 # Deterministic output
            )

            mock_request = None

            # Call the chat completion logic
            response = await self.openai_chat(health_request, mock_request)

            # Check if the response indicates success (status code 200)
            if response.status_code == 200:
                return Response(status_code=200, content="Generation health check OK")
            else:
                logger.error(f"Health generate check failed with status code: {response.status_code}")
                try:
                    # Attempt to get body for more details if possible
                    body = response.body if hasattr(response, 'body') else await response.body()
                    logger.error(f"Health generate check response body: {body}")
                except Exception:
                    pass # Ignore errors trying to get body details
                return Response(status_code=500, content="Generation health check failed")

        except Exception as e:
            logger.error(f"Health generate check encountered exception: {e}", exc_info=True)
            return Response(status_code=500, content=f"Generation health check failed: {str(e)}")

    async def version(self) -> JSONResponse:
        ver = {"version": VERSION}
        return JSONResponse(content=ver)

    async def get_model(self) -> JSONResponse:
        model_list = ModelList(data=[ModelCard(id=self.model)])
        return JSONResponse(content=model_list.model_dump())

    async def get_iteration_stats(self) -> JSONResponse:
        stats = []
        async for stat in self.llm.get_stats_async(2):
            stats.append(stat)
        return JSONResponse(content=stats)

    async def get_kv_cache_events(self) -> JSONResponse:
        events = []
        try:
            async for event in self.llm.get_kv_cache_events_async(2):
                events.append(event)
        except IndexError:
            # queue is empty, no more events
            pass
        return JSONResponse(content=events)

    async def openai_chat(self, request: ChatCompletionRequest, raw_request: Request) -> Response:

        def get_role() -> str:
            if request.add_generation_prompt:
                role = "assistant"
            else:
                role = request.messages[-1]["role"]
            return role

        async def chat_stream_generator(
                promise: RequestOutput, postproc_params: PostprocParams) -> AsyncGenerator[str, None]:
            if not self.postproc_worker_enabled:
                post_processor, args = postproc_params.post_processor, postproc_params.postproc_args
            async for res in promise:
                pp_results = res.outputs[0]._postprocess_result if self.postproc_worker_enabled else post_processor(res, args)
                if res.finished and self.metrics_collector:
                    self.metrics_collector.log_metrics_dict(res.metrics_dict)
                for pp_res in pp_results:
                    yield pp_res
            yield "data: [DONE]\n\n"
            nvtx_mark("generation ends")

        async def create_chat_response(
                promise: RequestOutput, postproc_params: PostprocParams, disaggregated_params: Optional[LlmDisaggregatedParams] = None) -> ChatCompletionResponse:
            await promise.aresult()
            if self.postproc_worker_enabled:
                chat_response =promise.outputs[0]._postprocess_result
            else:
                post_processor, args = postproc_params.post_processor, postproc_params.postproc_args
                chat_response = post_processor(promise, args)

            # Add prompt_tokens_ids to the response
            if disaggregated_params and disaggregated_params.request_type and disaggregated_params.request_type == "context_only":
                chat_response.prompt_token_ids = promise.prompt_token_ids
            if promise.finished and self.metrics_collector:
                self.metrics_collector.log_metrics_dict(promise.metrics_dict)
            return chat_response

        try:
            check_multiple_response(request.n, self.llm.args.backend)
            conversation: List[ConversationMessage] = []
            tool_dicts = None if request.tools is None else [
                tool.model_dump() for tool in request.tools
            ]
            # Pass the tokenizer vocabulary size so ``logit_bias`` can be
            # expanded into an embedding bias tensor in the sampler.
            sampling_params = request.to_sampling_params(
                vocab_size=self.tokenizer.tokenizer.vocab_size)
            # TODO: better way to enable metrics
            if len(os.getenv("TRTLLM_KVCACHE_TIME_OUTPUT_PATH", "")) > 0:
                sampling_params.return_perf_metrics = True
            postproc_args = ChatPostprocArgs.from_request(request)
            disaggregated_params = to_llm_disaggregated_params(request.disaggregated_params)

            conversation, mm_coroutines, mm_placeholder_counts = parse_chat_messages_coroutines(request.messages, self.model_config)

            if request.prompt_token_ids is not None:
                prompt = request.prompt_token_ids
            else:
                prompt: str = apply_chat_template(
                    model_type=self.model_config.model_type,
                    tokenizer=self.tokenizer,
                    processor=self.processor,
                    conversation=conversation,
                    add_generation_prompt=request.add_generation_prompt,
                    mm_placeholder_counts=mm_placeholder_counts,
                    tools=tool_dicts,
                    documents=request.documents,
                    chat_template=request.chat_template,
                    chat_template_kwargs=request.chat_template_kwargs or {},
                )
            prompt = prompt_inputs(prompt)

            mm_data = await mm_coroutines
            if mm_data is not None:
                prompt["multi_modal_data"] = mm_data

            postproc_args.reasoning_parser = self.llm.args.reasoning_parser
            if conversation and conversation[-1].get(
                    "content") and conversation[-1].get("role") == get_role():
                postproc_args.last_message_content = conversation[-1]["content"]
            postproc_params = PostprocParams(
                post_processor=chat_stream_post_processor
                if request.stream else chat_response_post_processor,
                postproc_args=postproc_args,
            )

            promise = self.llm.generate_async(
                inputs=prompt,
                sampling_params=sampling_params,
                _postproc_params=postproc_params if self.postproc_worker_enabled else None,
                streaming=request.stream,
                lora_request=request.lora_request,
                disaggregated_params=disaggregated_params
            )
            asyncio.create_task(self.await_disconnected(raw_request, promise))
            if not self.postproc_worker_enabled:
                postproc_args.tokenizer = self.tokenizer
                postproc_args.num_prompt_tokens = len(promise.prompt_token_ids)

            if request.stream:
                response_generator = chat_stream_generator(promise, postproc_params)
                return StreamingResponse(content=response_generator,
                                         media_type="text/event-stream")
            else:
                response = await create_chat_response(promise, postproc_params, disaggregated_params)
                return JSONResponse(content=response.model_dump())
        except CppExecutorError:
            logger.error(traceback.format_exc())
            # If internal executor error is raised, shutdown the server
            signal.raise_signal(signal.SIGINT)
        except Exception as e:
            logger.error(traceback.format_exc())
            return self.create_error_response(str(e))

    async def chat_harmony(self, request: ChatCompletionRequest, raw_request: Request) -> Response:
        """
        Chat Completion API with harmony format support.
        Supports both streaming and non-streaming modes.
        """
        # try:
        #     # 1. RAW INPUT REQUEST (before harmony adapter)
        #     # Convert Pydantic models to dictionaries for JSON serialization (standard pattern)
        # print("Run chat_harmony\n")
        # print(f"request\n\n{request}\n")
        tools_dict = None
        if request.tools:
            tools_dict = [tool.model_dump() for tool in request.tools]

        # Convert OpenAI messages to harmony tokens
        # Convert Pydantic models to dictionaries for harmony adapter (standard pattern)
        messages_dict = []
        for msg in request.messages:
            if hasattr(msg, 'model_dump'):
                messages_dict.append(msg.model_dump())
            else:
                messages_dict.append(msg)

        # Reasoning effort precedence: request.reasoning_effort > system message parsing > serving default
        reasoning_effort = request.reasoning_effort

        # Pass explicit reasoning effort to adapter to override system message parsing
        explicit_reasoning_effort = reasoning_effort if reasoning_effort != self.harmony_adapter.default_reasoning_effort else None

        # Get tool_choice from request
        tool_choice = getattr(request, 'tool_choice', None)

        harmony_tokens = self.harmony_adapter.openai_to_harmony_tokens(
            messages_dict,
            tools_dict,
            explicit_reasoning_effort=explicit_reasoning_effort,
            tool_choice=tool_choice
        )

        # Create engine prompt
        # engine_prompt = {
        #     "prompt_token_ids": harmony_tokens,
        #     "prompt": "",  # We use token IDs directly
        #     "multi_modal_data": None,
        # }

        # Get harmony stop tokens
        harmony_stop_tokens = self.harmony_adapter.get_stop_tokens()
        if request.stop_token_ids:
            request.stop_token_ids.extend(harmony_stop_tokens)
        else:
            request.stop_token_ids = harmony_stop_tokens

        sampling_params = request.to_sampling_params(
            vocab_size=self.tokenizer.tokenizer.vocab_size)

        # Generate
        generator = self.llm.generate_async(
            inputs=harmony_tokens,
            sampling_params=sampling_params,
            streaming=request.stream,
            lora_request=request.lora_request,
        )

        # Create request ID
        request_id = str(generator.request_id)

        # Handle streaming
        if request.stream:
            return StreamingResponse(self._handle_streaming_response(generator, request_id, request), media_type="text/event-stream")
        else:
            return await self._handle_non_streaming_response(generator, request_id, request)

        # except Exception as e:
        #     logger.error("Error in harmony chat completion: %s", e)
        #     logger.debug("Error details: %s", traceback.format_exc())
        #     return self.create_error_response(message=str(e), err_type="internal_error")

    async def _handle_streaming_response(self, generator, request_id: str, request: ChatCompletionRequest) -> AsyncGenerator[str, None]:
        """Handle streaming response with harmony format."""
        try:
            async for res in generator:
                if res.outputs:
                    output = res.outputs[0]

                    ## DEBUG: Show raw model output tokens and decoded text
                    # raw_tokens = output.token_ids
                    # raw_text = self.harmony_adapter.encoding.decode_utf8(raw_tokens)
                    # logger.debug("RAW MODEL STREAMING OUTPUT: %d tokens", len(raw_tokens))
                    # logger.debug("Raw decoded text: %r", raw_text)

                    # Convert tools to dictionary format for harmony adapter (standard pattern)
                    tools_dict = None
                    if request.tools:
                        tools_dict = [tool.model_dump() for tool in request.tools]

                    # Get tool_choice from request - if "none", don't pass tools to parser
                    tool_choice = getattr(request, 'tool_choice', None)
                    if tool_choice == "none":
                        tools_for_parser = None
                    else:
                        tools_for_parser = tools_dict

                    # Create OpenAI streaming responses
                    try:
                        responses = self.harmony_adapter.create_openai_streaming_response(
                            request_id=request_id,
                            tokens=output.token_ids,
                            available_tools=tools_for_parser,
                            model_name=request.model or "harmony-model",
                            tool_choice=tool_choice
                        )
                        logger.debug("Generated streaming responses: %s", responses)

                        # Yield each properly formatted response
                        for response in responses:
                            yield response

                    except Exception as e:
                        logger.error("Failed to create OpenAI streaming response: %s", e)
                        logger.debug("Streaming error details: %s", traceback.format_exc())
                        # Continue without yielding anything for this batch

            # Send final message with finish_reason
            final_response = ChatCompletionStreamResponse(
                id=request_id,
                created=int(time.time()),
                model=request.model or "harmony-model",
                choices=[ChatCompletionResponseStreamChoice(
                    index=0,
                    delta=DeltaMessage(),
                    finish_reason="stop"
                )]
            )

            yield f"data: {final_response.model_dump_json(exclude_unset=True)}\n\n"
            yield "data: [DONE]\n\n"

        except Exception as e:
            logger.error("Error in streaming response: %s", e)
            logger.debug("Streaming error details: %s", traceback.format_exc())
            # Send error response
            error_response = ChatCompletionStreamResponse(
                id=request_id,
                created=int(time.time()),
                model=request.model or "harmony-model",
                choices=[ChatCompletionResponseStreamChoice(
                    index=0,
                    delta=DeltaMessage(content=f"[Error: {str(e)}]"),
                    finish_reason="error"
                )]
            )
            yield f"data: {error_response.model_dump_json(exclude_unset=True)}\n\n"
            yield "data: [DONE]\n\n"

    async def _handle_non_streaming_response(self, generator, request_id: str, request: ChatCompletionRequest) -> ChatCompletionResponse:
        """Handle non-streaming response with harmony format."""
        try:
            # Get final result
            final_res: Optional[RequestOutput] = None
            # model_start_time = time.time()
            async for res in generator:
                final_res = res

            if final_res is None:
                raise RuntimeError("No output generated")

            # 3. RAW MODEL OUTPUT
            logger.debug("================================================")
            logger.debug("RAW MODEL OUTPUT:")
            logger.debug(final_res.outputs)
            logger.debug("================================================")
            # raw_output_tokens = final_res.outputs[0].token_ids
            # raw_output_text = self._safe_decode_utf8(raw_output_tokens, "RAW_OUTPUT: ")
            # logger.info(" RAW MODEL OUTPUT: %d tokens\n%s", len(raw_output_tokens), raw_output_text)

            # Parse harmony output to OpenAI format
            # Convert tools to dictionary format for harmony adapter (standard pattern)
            tools_dict = None
            if request.tools:
                tools_dict = [tool.model_dump() for tool in request.tools]

            # Get tool_choice from request - if "none", don't pass tools to parser
            tool_choice = getattr(request, 'tool_choice', None)
            if tool_choice == "none":
                tools_for_parser = None
            else:
                tools_for_parser = tools_dict

            parsed_output, tool_call_parsing_failed = self.harmony_adapter.harmony_output_to_openai(
                final_res.outputs[0].token_ids, tools_for_parser, tool_choice
            )
            if tool_call_parsing_failed:
                print(f"request\n\n{request}")

            # 4. CONVERTED OUTPUT (after harmony to openai conversion)
            logger.debug("✅ CONVERTED OUTPUT: %s", json.dumps(parsed_output, indent=2))

            # Optional: Log if harmony parsing failed (for debugging)
            if parsed_output.get('_harmony_parsing_failed'):
                logger.warning("⚠️ Harmony parsing fell back to raw text decoding")

            # Create response message
            response_message = self._create_response_message(parsed_output)

            # Determine finish reason
            finish_reason = self._determine_finish_reason(parsed_output)

            # Create usage info from metrics (RequestOutput doesn't have usage in v1)
            usage_info = self._create_usage_info(final_res, request)

            # Create response
            response = ChatCompletionResponse(
                id=request_id,
                created=int(time.time()),
                model=request.model or "harmony-model",
                choices=[{
                    "index": 0,
                    "message": response_message,
                    "finish_reason": finish_reason
                }],
                usage=usage_info,
            )
            if tool_call_parsing_failed:
                print(f"response\n\n{response}\n")

            return response

        except Exception as e:
            logger.error("Error in non-streaming response: %s", e)
            logger.debug("Non-streaming error details: %s", traceback.format_exc())
            return self.create_error_response(message=str(e), err_type="internal_error")

    def _create_response_message(self, parsed_output: dict[str, Any]) -> dict[str, Any]:
        """Create response message from parsed harmony output."""
        message = {
            "role": parsed_output.get("role", "assistant"),
            "content": parsed_output.get("content")
        }

        # Add tool_calls if present
        if "tool_calls" in parsed_output:
            message["tool_calls"] = parsed_output["tool_calls"]

        # Add reasoning_content if present
        if "reasoning_content" in parsed_output:
            message["reasoning_content"] = parsed_output["reasoning_content"]
            message["reasoning"] = parsed_output["reasoning_content"]

        return message

    def _determine_finish_reason(self, parsed_output: dict[str, Any]) -> str:
        """Determine finish reason based on parsed output."""
        if "tool_calls" in parsed_output and parsed_output["tool_calls"]:
            return "tool_calls"
        else:
            return "stop"

    def _has_tool_calls(self, message: dict[str, Any]) -> bool:
        """Check if message has tool calls using Option 3 mapping."""
        return "tool_calls" in message and bool(message["tool_calls"])

    def _create_usage_info(self, final_res: RequestOutput, request: ChatCompletionRequest) -> UsageInfo:
        """Create usage info from RequestOutput following serving_chat.py pattern."""
        # Calculate prompt tokens from prompt_token_ids and encoder_prompt_token_ids
        assert final_res.prompt_token_ids is not None
        num_prompt_tokens = len(final_res.prompt_token_ids)

        # Calculate completion tokens from all outputs
        num_generated_tokens = sum(
            len(output.token_ids) for output in final_res.outputs)

        # Create usage info
        usage = UsageInfo(
            prompt_tokens=num_prompt_tokens,
            completion_tokens=num_generated_tokens,
            total_tokens=num_prompt_tokens + num_generated_tokens
        )

        return usage

    async def openai_completion(self, request: CompletionRequest, raw_request: Request) -> Response:

        async def completion_response(promise: RequestOutput,
                                      postproc_params: Optional[PostprocParams]) -> CompletionResponse:
            response = await promise
            if not self.postproc_worker_enabled:
                post_processor, args = postproc_params.post_processor, postproc_params.postproc_args
                pp_result = post_processor(response, args)
            else:
                pp_result = response.outputs[0]._postprocess_result
            if disaggregated_params and disaggregated_params.request_type and disaggregated_params.request_type == "context_only":
                # Include prompt token ids for context-only requests
                pp_result.prompt_token_ids = response.prompt_token_ids
            if response.finished and self.metrics_collector:
                self.metrics_collector.log_metrics_dict(response.metrics_dict)
            return pp_result

        def merge_completion_responses(responses: List[CompletionResponse]) -> CompletionResponse:
            all_choices: List[CompletionResponseChoice] = []
            all_prompt_token_ids: List[List[int]] = []
            num_prompt_tokens = num_gen_tokens = 0
            for rsp in responses:
                choices, usage = rsp.choices, rsp.usage
                all_choices.extend(choices)
                num_prompt_tokens += usage.prompt_tokens
                num_gen_tokens += usage.completion_tokens
                # Aggregate prompt token ids for context-only requests
                if rsp.prompt_token_ids is not None:
                    all_prompt_token_ids.append(rsp.prompt_token_ids)

            usage_info = UsageInfo(
                prompt_tokens=num_prompt_tokens,
                completion_tokens=num_gen_tokens,
                total_tokens=num_gen_tokens + num_prompt_tokens,
            )
            merged_rsp = CompletionResponse(
                model=self.model,
                choices=all_choices,
                usage=usage_info,
                prompt_token_ids=all_prompt_token_ids,
            )
            return merged_rsp

        async def completion_generator(promise: RequestOutput, params: Optional[PostprocParams]):
            async for output in promise:
                if not self.postproc_worker_enabled:
                    post_processor, args = params.post_processor, params.postproc_args
                    pp_result = post_processor(output, args)
                else:
                    pp_result = output.outputs[0]._postprocess_result
                if output.finished and self.metrics_collector:
                    self.metrics_collector.log_metrics_dict(output.metrics_dict)
                for pp_res in pp_result:
                    yield pp_res

        async def merge_generators(generators: List[AsyncIterator[Any]]):
            result_queue = asyncio.Queue()
            finished = [False] * len(generators)

            async def producer(generator: AsyncIterator[Any], idx: int):
                async for output in generator:
                    await result_queue.put(output)
                finished[idx] = True

            tasks = [
                asyncio.create_task(producer(generator, idx)) for idx, generator in enumerate(generators)
            ]

            while not all(finished) or not result_queue.empty():
                output = await result_queue.get()
                yield output
            await asyncio.gather(*tasks)

        async def generator_wrapper(generator: AsyncIterator[Any]):
            async for output in generator:
                yield output
            yield "data: [DONE]\n\n"

        try:
            check_multiple_response(request.n, self.llm.args.backend)
            if isinstance(request.prompt, str) or \
                (isinstance(request.prompt, list) and isinstance(request.prompt[0], int)):
                prompts = [request.prompt]
            else:
                prompts = request.prompt

            promises: List[RequestOutput] = []
            postproc_params_collection: List[Optional[PostprocParams]] = []
            # Pass the tokenizer vocabulary size so ``logit_bias`` can be
            # expanded into an embedding bias tensor in the sampler.
            sampling_params = request.to_sampling_params(
                vocab_size=self.tokenizer.tokenizer.vocab_size)
            # TODO: better way to enable metrics
            if len(os.getenv("TRTLLM_KVCACHE_TIME_OUTPUT_PATH", "")) > 0:
                sampling_params.return_perf_metrics = True
            disaggregated_params = to_llm_disaggregated_params(request.disaggregated_params)
            for idx, prompt in enumerate(prompts):
                postproc_args = CompletionPostprocArgs.from_request(request)
                postproc_args.prompt_idx = idx
                if request.echo:
                    postproc_args.prompt = prompt
                postproc_params = PostprocParams(
                    post_processor=completion_stream_post_processor
                    if request.stream else completion_response_post_processor,
                    postproc_args=postproc_args,
                )
                promise = self.llm.generate_async(
                    inputs=prompt,
                    sampling_params=sampling_params,
                    _postproc_params=postproc_params,
                    streaming=request.stream,
                    lora_request=request.lora_request,
                    disaggregated_params=disaggregated_params
                )
                asyncio.create_task(self.await_disconnected(raw_request, promise))
                if not self.postproc_worker_enabled:
                    postproc_args.tokenizer = self.tokenizer
                    postproc_args.num_prompt_tokens = len(promise.prompt_token_ids)
                promises.append(promise)
                postproc_params_collection.append(None if self.postproc_worker_enabled else postproc_params)

            if request.stream:
                generators = [completion_generator(promise, params)
                              for promise, params in zip(promises, postproc_params_collection)]
                response_generator = merge_generators(generators) if len(promises) > 1 else generators[0]
                return StreamingResponse(content=generator_wrapper(response_generator),
                                            media_type="text/event-stream")
            else:
                rsps = await asyncio.gather(*[completion_response(promise, params)
                                              for promise, params in zip(promises, postproc_params_collection)])
                response = merge_completion_responses(rsps) if len(rsps) > 1 else rsps[0]
                return JSONResponse(content=response.model_dump())
        except CppExecutorError:
            logger.error(traceback.format_exc())
            # If internal executor error is raised, shutdown the server
            signal.raise_signal(signal.SIGINT)
        except Exception as e:
            logger.error(traceback.format_exc())
            return self.create_error_response(str(e))

    async def openai_responses(self, request: ResponsesRequest, raw_request: Request) -> Response:
        def finish_reason_mapping(finish_reason: str) -> str:
            match finish_reason:
                case 'stop':
                    return 'completed'
                case 'length':
                    return 'incomplete'
                case 'timeout':
                    return 'failed'
                case 'cancelled':
                    return 'cancelled'

            raise RuntimeError("Should never reach here!")

        async def request_preprocess(request: ResponsesRequest, prev_response: Optional[ResponsesResponse]):
            tools_dict = None if request.tools is None else [
                tool.model_dump() for tool in request.tools
            ]
            # TODO: fix default_max_tokens
            sampling_params = request.to_sampling_params(
                default_max_tokens=int(16384),
                default_sampling_params={
                    "stop_token_ids": self.harmony_adapter.get_stop_tokens()
                })

            logger.debug("--- stop tokens: ---")
            logger.debug(self.harmony_adapter.get_stop_tokens())
            logger.debug("--------------------")

            # TODO: better way to enable metrics
            if len(os.getenv("TRTLLM_KVCACHE_TIME_OUTPUT_PATH", "")) > 0:
                sampling_params.return_perf_metrics = True

            if self.use_harmony:
                prev_msgs = []
                if self.enable_store:
                    prev_msgs = await self.conversation_store.get_conversation_history(prev_response_id)

                    logger.debug(f"Prev msgs:")
                    for msg in prev_msgs:
                        logger.debug(f" -> {msg.to_json()}")

                messages = construct_harmony_messages(request, prev_response, prev_msgs=prev_msgs)

                if self.enable_store and request.store:
                    await self.conversation_store.store_messages(request.request_id, messages, prev_response_id)

                input_tokens = render_for_completion(messages)
            else:
                logger.warning("Only gpt-oss is supported for responses api for now!")

            logger.debug("======= Complete Inputs to model =======")
            logger.debug(decode_tokens(input_tokens))
            logger.debug("========================================")
            return input_tokens, sampling_params, tools_dict

        async def create_response(generator, request: ResponsesRequest, tools_dict, sampling_params) -> ResponsesResponse:
            final_res: Optional[RequestOutput] = None
            response_creation_time = int(time.time())
            async for res in generator:
                final_res = res

            if final_res is None:
                raise RuntimeError("No output generated")

            logger.debug("================================================")
            logger.debug("RAW MODEL OUTPUT:")
            logger.debug(final_res.outputs)
            logger.debug("================================================")

            output_messages = parse_output_tokens(final_res.outputs[0].token_ids)

            logger.debug(f"output messages: {len(output_messages)}")
            for msg in output_messages:
                logger.debug(f" -> {msg.to_json()}")

            # prepare responses output
            output_content = []
            for msg in output_messages:
                output_content.extend(parse_output_message(msg))

            response = ResponsesResponse.from_request(
                request=request,
                sampling_params=sampling_params,
                model_name=self.model_config.model_type,
                created_time=response_creation_time,
                output=output_content,
                status=finish_reason_mapping(final_res.outputs[0].finish_reason),
            )

            if self.enable_store and request.store:
                await self.conversation_store.store_response(
                    resp=response,
                    resp_msgs=output_messages,
                    prev_resp_id=prev_response_id)

            logger.debug("========== Response ===========")
            logger.debug(response)
            logger.debug("===============================")
            return response

        try:
            logger.debug(request)
            if request.background:
                # TODO: impl background request processing
                pass

            # Get prev response
            prev_response = None
            if self.enable_store:
                prev_response_id = request.previous_response_id
                if prev_response_id is not None:
                    if not prev_response_id.startswith("resp_"):
                        return self._create_invalid_response_id_error(prev_response_id)

                    prev_response = await self.conversation_store.load_response(prev_response_id)
                    if prev_response is None:
                        logger.debug(f"response_id {prev_response_id} not found")
                        return self._create_response_id_not_found_error(prev_response_id)

            input_tokens, sampling_params, tools_dict = await request_preprocess(request, prev_response)

            generator = self.llm.generate_async(
                inputs=input_tokens,
                sampling_params=sampling_params,
                streaming=request.stream,
            )

            asyncio.create_task(self.await_disconnected(raw_request, generator))

            if request.stream:
                return self.create_error_response("Streaming is not supported yet!")
            else:
                return await create_response(generator, request, tools_dict, sampling_params)
        except CppExecutorError:
            logger.error(traceback.format_exc())
            # If internal executor error is raised, shutdown the server
            signal.raise_signal(signal.SIGINT)
        except Exception as e:
            logger.error(traceback.format_exc())
            return self.create_error_response(str(e))

        return JSONResponse(content={"detail": "None"})


    async def __call__(self, host, port):
        # Store the binding address for server registration
        self.binding_addr = f"http://{host}:{port}"
        config = uvicorn.Config(self.app,
                                host=host,
                                port=port,
                                log_level="info",
                                timeout_keep_alive=TIMEOUT_KEEP_ALIVE)
        await uvicorn.Server(config).serve()
